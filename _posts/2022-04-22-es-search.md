---
layout: post
title: "Elasticsearch：search"
date: 2022-04-22 00:16:17 +0800
categories: elasticsearch
tags: elasticsearch
---

Elasticsearch，主要目的就是search。最简单的寻找数据的方式是遍历，缺点是耗时；es搜索非常迅速，因为在存储上做了很多支持，通俗地说就是以空间换时间。除了快，es还支持种类繁多的各式搜索，以实现不同的目的。

1. Table of Contents, ordered
{:toc}

# 搜索
es有两种搜索方式：
1. 常用的`_search` api：https://www.elastic.co/guide/en/elasticsearch/reference/current/search-search.html
2. 直接按照`_id`获取文档：`GET <index>/_doc/<_id>`；

search api是最常用的搜索，功能很丰富，但它是 **从磁盘搜索**，所以新写入的数据必须在refresh后才能被搜索到，**仅仅在buffer pool里是不能被搜索的**。

> 关于refresh和buffer pool：[Elasticsearch：deep dive]({% post_url 2022-05-05-es-deep-dive %})

- https://www.elastic.co/guide/cn/elasticsearch/guide/current/making-text-searchable.html
> 早期的全文检索会为整个文档集合建立一个很大的倒排索引并将其 **写入到磁盘**。 一旦新的索引就绪，旧的就会被其替换，这样最近的变化便可以被检索到。

按照_id搜索，是可以直接搜索到的，和refresh interval无关。

## 按`_id`搜索
之所以和refresh无关，**因为`_id`不需要走Lucene索引：_id is realtime, not near-realtime**。

- https://www.reddit.com/r/elasticsearch/comments/oqkyom/comment/h6cfnlr/?utm_source=share&utm_medium=web2x&context=3

realtime get: regardless of the "refresh rate"

- https://github.com/elastic/elasticsearch/issues/1060
- https://elasticsearch.cn/question/4278

> query是走倒排索引查询的所以会出现refresh未执行搜索不到的情况，但是如果你用get，也就是通过id查询的话，他会先从translog拿一下，写translog是写操作的第一步，就不受refresh影响了。2.2的代码我看了是这么做的，但是5.x不太清楚了

举个例子：
```
PUT /my_logs
{
  "settings": {
    "refresh_interval": "30s" 
  }
}

PUT my_logs/_doc/1
{
  "name": "1",
  "age": 233
}
```
直接使用_id是可以搜到的，虽然还没到30s：
```
GET my_logs/_doc/1
```
search api不行：
```
GET my_logs/_search
```
同理，update by query因为也要用到query，所以其实是改不到这个文档的：
```
POST my_logs/_update_by_query
{
  "query": {
    "term": {
      "name.keyword": {
        "value": "1"
      }
    }
  },
  "script": {
    "source": "ctx['name'] = '2'",
    "lang": "painless"
  }
}
```
但是使用_id则可以直接更新：
```
POST my_logs/_update/1
{
  "doc": {
    "name": "2"
  }
}
```
所以按照_id直接搜索、直接更新是可以的，和refresh interval无关。

`_search`和`_update_by_query`都是受影响的。写入之后，如果没有refresh，update by qeury返回结果是0，找不到要更新的文件：
```
{
  "took" : 1,
  "timed_out" : false,
  "total" : 0,
  "updated" : 0,
  "deleted" : 0,
  "batches" : 0,
  "version_conflicts" : 0,
  "noops" : 0,
  "retries" : {
    "bulk" : 0,
    "search" : 0
  },
  "throttled_millis" : 0,
  "requests_per_second" : -1.0,
  "throttled_until_millis" : 0,
  "failures" : [ ]
}
```

search api也可以使用`_id` field，**这种search虽然用了_id，但是因为用的是search api，决定了它要去磁盘上找文档**，所以自然也搜不到：
```
GET my_logs/_search
{
  "query": {
    "term": {
      "_id": {
        "value": "1"
      }
    }
  }
}
```
> 因为_id默认也被加到Lucene里了，为了方便搜索。
> - https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-id-field.html
>
> Each document has an _id that uniquely identifies it, which is indexed so that documents can be looked up either with the GET API or the ids query. 

## 按field搜索：search api
search api虽然有refresh interval限制，但是它强啊！

search有url形式的api，可以直接通过参数传递搜索的条件。但随着搜索变得复杂，还是使用body传搜索条件更清晰。

## term - 完整匹配
term用于搜索keyword类型。在[Elasticsearch：basic]({% post_url 2022-04-20-es-basic %})中介绍过，keyword类型的field是囫囵存储，不会把字符串内容拆分为token。

- https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-term-query.html

**term有一个常见误区：如果对text type的field使用term查询，往往搜不到文档**。比如存储的是John，使用term John搜索搜不到这条文档，因为text默认会把John处理为小写形式，所以term john才能搜到这条文档。所以对text类型，不要用term！使用match！

## terms - mysql `in`
term是拿文档和一个值进行匹配，terms是和一堆值进行匹配，匹配上其中一个就可以。类似于mysql的in。

- https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-terms-query.html

它也有`minimum_should_match`，和bool的should一样，指定文档和目标值们至少匹配的几个才算匹配。

## match - 多词查询
match用于搜索text类型，做的是token上的匹配。text会把存储的字符串拆分为token做倒排索引，match会对搜索字符串做同样的拆分，拿其中的每一个token和已存储的倒排索引做匹配。

match各个token之间是“或”的关系：只要有一个token和文档匹配就算匹配。但是也可以提高精度：
- `operator`：**选择and，就是“与”的关系**；
- `minimum_should_match`：介于“或”和“与”之间，可以设置数字（匹配词个数）或者百分比，比如有75%的词匹配就行（如果75%的词不是整数个，es会取整）；

Ref:
- https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-match-query.html
- https://www.elastic.co/guide/cn/elasticsearch/guide/current/multi-match-query.html

## match_phrase - 考虑token位置
短语匹配，会比match匹配更精准。文档里必须有这个短语才行。**短语匹配不仅使用token，还使用token的位置**：比如“hello pikachu”，不仅要匹配到hello和pikachu，pikachu的position还必须比hello大1，才算匹配。
- https://www.elastic.co/guide/cn/elasticsearch/guide/current/phrase-matching.html#_%E4%BB%80%E4%B9%88%E6%98%AF%E7%9F%AD%E8%AF%AD

match phrase很严格，有可能导致匹配到的文档条目过少。可以设置slop属性，允许短语之间的单词出现一定的间隔、错位：
- https://www.elastic.co/guide/cn/elasticsearch/guide/current/slop.html

最终的排序结果，仍然是离得近的排序更靠前。

- https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-match-query-phrase.html
- https://www.elastic.co/guide/cn/elasticsearch/guide/current/phrase-matching.html

### match + match_phrase - 混合起来搜索
match太松，match_phrase太严，把他们俩混合起来，打分叠加，查到的内容可能会更符合期待：
- https://www.elastic.co/guide/cn/elasticsearch/guide/current/proximity-relevance.html

### shingle - 介于match和match_phrase之间
见后文。

## bool - 组合搜索
bool是多种搜索的组合：
- https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-bool-query.html

支持与或非，
- must：与；
- should：或；
- must_not：非；
- filter：**也是与。但是该query不计入score，所以叫“过滤”。由于不需要计算分数，所以filter比match query快**。

每一个与或非内，也是一个query的term或者match：
```
POST _search
{
  "query": {
    "bool" : {
      "must" : {
        "term" : { "user.id" : "kimchy" }
      },
      "filter": {
        "term" : { "tags" : "production" }
      },
      "must_not" : {
        "range" : {
          "age" : { "gte" : 10, "lte" : 20 }
        }
      },
      "should" : [
        { "term" : { "tags" : "env1" } },
        { "term" : { "tags" : "deployed" } }
      ],
      "minimum_should_match" : 1,
      "boost" : 1.0
    }
  }
}
```
**和match（token之间的或）一样，bool也支持`minimum_should_match`，代表或语句（should）至少要命中几个**：
> `minimum_should_match`的默认值是不定的：If the bool query includes at least one should clause and no must or filter clauses, the default value is 1. Otherwise, the default value is 0.

**任何一个查询语句都可以用`boost`属性给打分加权**：
- https://www.elastic.co/guide/cn/elasticsearch/guide/current/_boosting_query_clauses.html

### filter缓存
**一个filter在该Lucene上的过滤结果可以通过二进制集合（类似Java中的bitset）表示，因为Lucene是不可变的，这个集合就可以被缓存下来，留待后续使用**。

## multi_match - 一种特殊的bool
如果对不同的field执行相同的搜索，可以直接用multi_match，而不必“使用多个match，再用bool组合起来”：
- https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-multi-match-query.html
- https://www.elastic.co/guide/cn/elasticsearch/guide/current/multi-match-query.html

## exists - 值不为空
- **The field in the source JSON is null or []**
- The field has "index" : false set in the mapping
- The length of the field value exceeded an ignore_above setting in the mapping
- The field value was malformed and ignore_malformed was defined in the mapping

这样的文档都不会被exists query搜索到。

**null和空对es来说没啥区别，最终结果都是不加入索引**：
- https://www.elastic.co/guide/en/elasticsearch/guide/current/_dealing_with_null_values.html

查询索引里有没有，相当于 where field is not not：
```
GET /index/_count
{
  "query": {
    "exists": {
      "field": "updateTime"
    }
  }
}
```

## not exists
“不存在”，missing已经deprecate了，用`must_not` + `exists`：
```
GET /index/_count
{
  "query": {
    "bool": {
      "must_not": [
        {
          "exists": {
            "field": "updateTime"
          }
        }
      ]
    }
  }
}
```

eg：把字段更新为null：
```
POST /puzzle/_update/_bCvS30BsRxrU7Dv8KYG
{
  "doc": {
    "article": null
  }
}
```

查询：
```
GET /puzzle/_search
{
  "query": {
    "term": {
      "_id": {
        "value": "_bCvS30BsRxrU7Dv8KYG"
      }
    }
  }
}
```
结果：
```
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 1,
      "relation" : "eq"
    },
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "puzzle",
        "_type" : "_doc",
        "_id" : "_bCvS30BsRxrU7Dv8KYG",
        "_score" : 1.0,
        "_source" : {
          "article" : null
        }
      }
    ]
  }
}
```
用 must_not + exists 也可以查出来：
```
GET /puzzle/_search
{
  "query": {
    "bool": {
      "must_not": [
        {
          "exists": {
            "field": "article"
          }
        }
      ]
    }
  }
}
```

- https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-exists-query.html

## range - 范围匹配
对数量的range query比较简单，主要是对time的range query比较复杂一些：
```
GET /index/_search
{
  "query": {
    "range": {
      "updateTime": {
        "gte": "2021-08-26",
        "lt": "2021-08-27",
        "format": "yyyy-MM-dd",
        "time_zone": "+08:00"
      }
    }
  },
  "sort": [
    {
      "timestamp": {
        "order": "desc"
      }
    }
  ]
}
```

```
GET /index/_count
{
  "query": {
    "range": {
      "timestamp": {
        "gte": "now-7d/d",
        "lt": "now/d",
        "time_zone": "+08:00"
      }
    }
  }
}
```
之前说了，es里的时间本质上是string或者long epoch milli。**在时间上，range搜索时支持不同时区时间自动转换**，所以写起来稍微麻烦一些。

- https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-range-query.html

## fuzzy - 自动纠错
即使输入词某些字母输错了，依旧可以匹配上正确的文档！

> To find similar terms, the fuzzy query creates a set of all possible variations, or expansions, of the search term within a specified edit distance. The query then returns exact matches for each expansion.

所以原本一个token会按照Levenshtein distance产生多个可能错误的token，然后分别和这些错误的token匹配。

大概需要匹配的索引翻了好几倍，所以如果`search.allow_expensive_queries`=false，不会执行。

- https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-fuzzy-query.html

## 返回随机结果
假设指定一个条件，想随机返回满足条件的任意一个文档，可以使用`function_score`的[`random_score`](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html#function-random)，同时使用filter做条件过滤，不考虑文档得分。**但是注意一定要修改`boost_mode`，默认用的是multiply，但因为有0分存在，`random_score`乘以0没什么用了。可以换成`sum`，最好使用`replace`**：
```
GET witake_media/_search
{
  "query": {
    "function_score": {
      "query": {
        "bool": {
          "filter": [
            {
              "term": {
                "platform": {
                  "value": "TikTok"
                }
              }
            },
            {
              "match": {
                "description.text": "prank"
              }
            }
          ]
        }
      },
      "random_score": {},
      "boost_mode": "replace"
    }
  },
  "size": 1
}
```

> `replace`: only function score is used, the query score is ignored

- https://stackoverflow.com/questions/9796470/random-order-pagination-elasticsearch

## 其他不太会用到的搜索
### query_string - 搜索所有field
**默认查询文档的所有field**（但不包括nested文档），除非指定了field。甚至可以在搜索词里指定AND/OR进行多条件查询。

示例：`name:nosql AND -description:mongodb`，搜索name里包含nosql，且description不包含mongodb的文档。

- https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html

两个显著缺点：
1. 不好阅读，不易维护和扩展；
2. 查询过于强大，如果直接接收用户输入作为搜索词，用户查询的权限就比较大；

### match_phrase_prefix - 短语前缀搜索
和match_phrase类似，不过最后一个词用前缀。比如“hello wor”可以匹配“hello world”，也可以匹配“hello word”。

- https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-match-query-phrase-prefix.html

**因为前面的词可以用倒排索引搜索，最后一个词要做前缀判断，所以会比match/match_phrase慢**。可以使用`max_expansions`指定最多匹配多少个，以缩短返回时间。默认是50个。

### prefix - 前缀搜索
前缀查询。默认不分析搜索词，所以大小写影响搜索结果。可以设置`case_insensitive`属性为true。

- https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-prefix-query.html

**但是它实在不够高效。所以如果`search.allow_expensive_queries`=false，不会执行prefix query**：
- https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-prefix-query.html#prefix-query-allow-expensive-queries

### index_prefix - 前缀索引搜索
**定义mapping的时候，给field做前缀索引，这样的话再用前缀搜，就可以和前缀索引匹配，就快了**。前缀的最小最大长度可以指定一下。

- https://www.elastic.co/guide/en/elasticsearch/reference/current/index-prefixes.html

> 和edge n-gram很像。

### wildcard - 通配符搜索
越来越离谱了……可以做通配符匹配搜索。慢是肯定的，而且因为通配符可以出现在任何地方，所以不像index_prefix还能拯救一下prefix，没有人能拯救wildcard搜索的速度。所以如果`search.allow_expensive_queries`=false，不会执行wildcard query。

- https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-wildcard-query.html

## 搜索选项
### `_source`
控制返回的`_source`字段。默认返回所有。如果不需要太多字段，只返回指定的field可以节省网络开销。甚至还支持通配符匹配field、include/exclude某些field：
- https://www.elastic.co/guide/en/elasticsearch/reference/current/search-search.html

# 倒排索引 - 空间换时间
es的搜索快，主要是因为提前建好了倒排索引，以空间换时间：
- https://www.elastic.co/guide/cn/elasticsearch/guide/current/inverted-index.html

遍历是O(n)操作，而倒排查询是O(1)操作。

es举了个倒排索引的例子——

假设有两个文档：
1. The quick brown fox jumped over the lazy dog
2. Quick brown foxes leap over lazy dogs in summer

按空格拆分后，构建出来的倒排索引就是这样的：
```
Term      Doc_1  Doc_2
-------------------------
Quick   |       |  X
The     |   X   |
brown   |   X   |  X
dog     |   X   |
dogs    |       |  X
fox     |   X   |
foxes   |       |  X
in      |       |  X
jumped  |   X   |
lazy    |   X   |  X
leap    |       |  X
over    |   X   |  X
quick   |   X   |
summer  |       |  X
the     |   X   |
------------------------
```
如果搜索quick brown，搜索到的结果：
```
Term      Doc_1  Doc_2
-------------------------
brown   |   X   |  X
quick   |   X   |
------------------------
Total   |   2   |  1
```

# analyzer - 文本预处理
但是仅仅倒排是不够的。比如搜索时如果想忽略大小写，一个单纯的倒排并不能做到这一点，必须在倒排之前，对数据进行处理，全部转为小写。

analyzer就是负责在建立倒排索引之前处理文本。analyzer由三部分组成：
1. `char_filter`：字符过滤器。**字符串在被分词之前，先对字符做一些过滤**。比如替换&为and、去掉html tag等；
2. `tokenizer`：分词器。**处理字符串，拆分为不同的token**；
3. `filter`：Token 过滤器（感觉起名为token filter会更贴切。可能es觉得本来就是在处理token，所以不用再提token了）。**改变token，也可以增删token**。比如token小写、转为词根、增加近义词。

es内置了一些analyzer：
- https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-analyzers.html

比如常用的standard analyzer，语言相关的english analyzer等。还可以添加第三方的比如ik analyzer，用于中文分词。

以standard analyzer和english analyzer为例分别介绍char_filter/tokenizer/filter：
- https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-standard-analyzer.html
- https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#english-analyzer

## char_filter
es内置的char filter不是很多，也不太常用：
- https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-charfilters.html

比如HTML Strip Character Filter。
- https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-htmlstrip-charfilter.html
- https://www.elastic.co/guide/cn/elasticsearch/guide/current/char-filters.html

**standard analyzer和english analyzer都没有使用char filter**。

> 尴尬……hhh

## tokenizer
内置的tokenizer：
- https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-tokenizers.html

**standard analyzer和english analyzer使用的都是standard tokenizer**，它使用空格给英文分词。汉语则是一个汉字分为一个词，失去了语义：
- https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-standard-tokenizer.html

**es可以使用analyze api非常方便地测试analyzer及其组件**：
```
POST _analyze
POST _analyze
{
  "tokenizer": "standard",
  "text": "hello123 456 world 你好吗"
}
```
输出：
```
{
  "tokens" : [
    {
      "token" : "hello123",
      "start_offset" : 0,
      "end_offset" : 8,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "456",
      "start_offset" : 9,
      "end_offset" : 12,
      "type" : "<NUM>",
      "position" : 1
    },
    {
      "token" : "world",
      "start_offset" : 13,
      "end_offset" : 18,
      "type" : "<ALPHANUM>",
      "position" : 2
    },
    {
      "token" : "你",
      "start_offset" : 19,
      "end_offset" : 20,
      "type" : "<IDEOGRAPHIC>",
      "position" : 3
    },
    {
      "token" : "好",
      "start_offset" : 20,
      "end_offset" : 21,
      "type" : "<IDEOGRAPHIC>",
      "position" : 4
    },
    {
      "token" : "吗",
      "start_offset" : 21,
      "end_offset" : 22,
      "type" : "<IDEOGRAPHIC>",
      "position" : 5
    }
  ]
}
```
可以看出标准分词器做的事情并不多，如果要考虑语义，则要使用更专业的分词器。

> **另外可以看出分出来的token除了token本身，还有位置信息、类型信息等，match_phrase会使用position**。

- `max_token_length`：默认255。标准分词器对token的长度是有限制的，超过255就在255出分词了。

> 注意：**平时说的icu和ik，其实是分词器tokenizer**。他们都提供了使用了该分词器的analyzer。比如ik_smart。
> 
> - https://www.elastic.co/guide/cn/elasticsearch/guide/current/icu-tokenizer.html

### pattern tokenizer
自己指定pattern，进行分词：
- https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-pattern-tokenizer.html

### UAX URL email tokenizer
给email和url分词：
- https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-uaxurlemail-tokenizer.html

注意默认最大的token是255，url可能会超。

### path hierarchy tokenizer
path和path的父目录都会是token：
- https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-pathhierarchy-tokenizer.html

## filter
token filter就比较多了，毕竟对token的处理需求还是很多的：
- https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-tokenfilters.html

比如lowercase filter：
- https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lowercase-tokenfilter.html

**使用analyze api指定tokenizer和filter测试一下：**
```
GET _analyze
{
  "tokenizer" : "standard",
  "filter" : ["lowercase"],
  "text" : "THE Quick FoX JUMPs"
}
```
输出：
```
[ the, quick, fox, jumps ]
```

**stop word filter，过滤掉语言中的stop word，比如a/an/the等无意义词**：
- https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-stop-tokenfilter.html

和tokenizer一样，**filter也有属性可以自定义**：
- https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-stop-tokenfilter.html#analysis-stop-tokenfilter-configure-parms

**standard analyzer使用了lowercase filter，english analyzer则使用了很多filter**：
```
PUT /english_example
{
  "settings": {
    "analysis": {
      "filter": {
        "english_stop": {
          "type":       "stop",
          "stopwords":  "_english_" 
        },
        "english_keywords": {
          "type":       "keyword_marker",
          "keywords":   ["example"] 
        },
        "english_stemmer": {
          "type":       "stemmer",
          "language":   "english"
        },
        "english_possessive_stemmer": {
          "type":       "stemmer",
          "language":   "possessive_english"
        }
      },
      "analyzer": {
        "rebuilt_english": {
          "tokenizer":  "standard",
          "filter": [
            "english_possessive_stemmer",
            "lowercase",
            "english_stop",
            "english_keywords",
            "english_stemmer"
          ]
        }
      }
    }
  }
}
```
第一个是`stop word filter`，**当不想要某些token时，可以把他们过滤掉**。这里设置的stop word集合是es内置的english stop word，`_english_`：
- https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-stop-tokenfilter.html#english-stop-words

最后两个filter是`stemmer filter`，**获取token的词根形式**，可以极大扩大匹配范围：
- https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-stemmer-tokenfilter.html

第二个是`keyword_marker filter`，**当不想把某个词作为词根时使用**：
- https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-keyword-marker-tokenfilter.html

### reverse token filter
颠倒分词：
- https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-reverse-tokenfilter.html

**看起来似乎没用，但是如果要匹配`*bar`，不如把token倒过来，再匹配`rab*`，速度极大提升**。

还有一种奇效：reverse + ngram + reverse：
```
GET /_analyze
{
  "tokenizer": "standard",
  "filter": ["reverse", "edge_ngram", "reverse"], 
  "text" : "Hello!"
}
```
从尾部生成edge_ngram。当然，和设置edge_ngram的`side=back`一个效果。

## 自定义analyzer
如果所有内置的analyzer都不符合自己的需求，就需要自定义一个analyzer。

改变标准analyzer的任何一点设置，都是在自定义analyzer。一般情况下自定义analyzer，其实就是：
1. **在标准char_filter/tokenizer/filter的基础上，改一改配置，生成一个新的char_filter/tokenizer/filter**；
2. **组装一个或多个char_filter/tokenizer/filter**；

> **上面贴的english analyzer，实际就是对tokenizer、filter改了改设置，组装起来的**。

比如自定义一个支持的token最大长度为5的standard analyzer：
```
PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "my_tokenizer"
        }
      },
      "tokenizer": {
        "my_tokenizer": {
          "type": "standard",
          "max_token_length": 5
        }
      }
    }
  }
}
```
也可以自定义一个char filter，把&转为and：
```
"char_filter": {
    "&_to_and": {
        "type":       "mapping",
        "mappings": [ "&=> and "]
    }
}
```
实际是在mapping char_filter的基础上，改了改配置。

- https://www.elastic.co/guide/cn/elasticsearch/guide/current/custom-analyzers.html

最后把这些char_filter/tokenizer/filter，放到custom类型的analyzer里，组装成一个自己的analyzer：
- https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-custom-analyzer.html

## analyzer的最小元素集合
一个analyzer虽然由三部分组成，但是char_filter和filter可以没有，也可以有多个（zero or more），**但是tokenizer必须有且只有一个（exactly one）**。

最典型的例子就是whitespace analyzer：
- https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-whitespace-analyzer.html

它只有一个tokenizer，whitespace tokenizer，按照空格分词。

# `_analyze` API
[测试analyzer，也可以测试char_filter、tokenizer、filter](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-analyze.html)：
```
GET /_analyze
{
  "tokenizer" : "keyword",
  "filter" : ["lowercase"],
  "char_filter" : ["html_strip"],
  "text" : "this is a <b>test</b>"
}
```
或者自定义一个truncate filter：
```
GET _analyze
{
  "tokenizer": "keyword",
  "filter": [
    {
      "type": "truncate",
      "length": 7
    },
    "lowercase"
  ],
  "text": [
    "Hello World"
  ]
}
```
返回：
```json
{
  "tokens" : [
    {
      "token" : "hello w",
      "start_offset" : 0,
      "end_offset" : 11,
      "type" : "word",
      "position" : 0
    }
  ]
}
```

**一个比较方便的功能是引用已有的index的某个field的analyzer**：
```
GET /<index>/_analyze
{
  "field": "description", 
  "text": ["hello world"]
}
```
或者直接指定analyzer名称，无论analyzer还是search_analyzer都可以：
```
GET <index>/_analyze
{
  "analyzer": "autocomplete_sentence_search",
  "text": ["hello world"]
}
```

# search as you type
有一些比较猛的filter，因为太强，所以单独拎出来说了。常用来做search as you type。

## n-gram filter - 字符滑动窗口
- https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-ngram-tokenfilter.html

## edge n-gram filter - 左窗口固定为edge的滑动窗口
edge ngram和ngram相比，**左窗口固定为edge的滑动窗口，所以它产生的都是前缀**：
- https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-edgengram-tokenfilter.html

token filter可以改变token，**也可以增删token**。stop word filter是删除token的例子，edge n-gram则是增加token的例子。

edge n-gram可以把一个token转成很多个token，每个token都是原本token的前缀：
```
GET _analyze
{
  "tokenizer": "standard",
  "filter": [
    { "type": "edge_ngram",
      "min_gram": 2,
      "max_gram": 4
    }
  ],
  "text": "hello world"
}
```
输出：
```
{
  "tokens" : [
    {
      "token" : "he",
      "start_offset" : 0,
      "end_offset" : 5,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "hel",
      "start_offset" : 0,
      "end_offset" : 5,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "hell",
      "start_offset" : 0,
      "end_offset" : 5,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "wo",
      "start_offset" : 6,
      "end_offset" : 11,
      "type" : "<ALPHANUM>",
      "position" : 1
    },
    {
      "token" : "wor",
      "start_offset" : 6,
      "end_offset" : 11,
      "type" : "<ALPHANUM>",
      "position" : 1
    },
    {
      "token" : "worl",
      "start_offset" : 6,
      "end_offset" : 11,
      "type" : "<ALPHANUM>",
      "position" : 1
    }
  ]
}
```
standard tokenizer把字符串分成hello和world两个token，edge n-gram filter则把他们映射为了6个token，每个token都是原有token的2-4个字符不等的前缀。

这样当我们输入字符的时候，只要搜索框在输入大于两个字符时就对es发起搜索请求，并把搜索结果实时展示在搜索框下，就会产生一种search as you type的效果！（如果搜索返回速度还赶不上用户输入速度，那就凉凉了……）

**因为这个功能太强了，所以es支持在定义mapping的时候，给某个field设为"search_as_you_type"类型**：
- https://www.elastic.co/guide/en/elasticsearch/reference/current/search-as-you-type.html

直接就能用，甚至都不需要自定义一个包含edge n-gram filter的analyzer了。

> 当输入“he wor”的时候，既匹配上了he，又匹配上了wor，该文档就会被作为候选项筛选出来。

### truncate token filter - 输入词过长
上述自定义的filter只生成了2-4长度的前缀作为token，如果用户输入了hello或者world，反而匹配不到这些前缀了，在用户看来这岂不是很离谱？

其本质原因是查询词超出了edge n-gram的`max_gram`。**这个时候可以给analyzer加一个truncate token filter使用，自动帮用户截断搜索词到`max_gram`的长度**，又可以搜到了：
- https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-edgengram-tokenfilter.html#analysis-edgengram-tokenfilter-max-gram-limits
- https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-truncate-tokenfilter.html

## edge n-gram tokenizer = keyword tokenizer + edge n-gram filter
一开始我还以为是我眼花了……后来发现真的有一个edge n-gram tokenizer，又有一个edge n-gram filter……

仔细想想也有道理：edge n-gram filter是别的tokenizer分好词后，把每个分词拆成前缀token。但是如果不想给句子分词，想直接构建一个句子的前缀，edge n-gram filter就做不到了。此时edge n-gram tokenizer就起到作用了！

**edge n-gram tokenizer默认把整个文本作为一个token**：
> With the default settings, **the edge_ngram tokenizer treats the initial text as a single token** and produces N-grams with minimum length 1 and maximum length 2

```
POST _analyze
{
  "tokenizer": "edge_ngram",
  "text": "hello world"
}
```
输出：
```
[ h, he ]
```

edge n-gram tokenizer除了可以配置`min_gram`、`min_gram`，**还可以配置`token_chars`数组，默认为空数组，即把所有文本当作一整个token，不拆分**：
- `token_chars`：可配置为`letter`/`digit`/`whitespace`等等。

**如果配置为whitespace，那这个edge n-gram tokenizer就和standard tokenizer + edge n-gram filter效果类似了**。

- https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-edgengram-tokenizer.html

最好也配合truncate token filter使用。

## completion suggester - search as you type
**如果按照乱序前缀匹配文档，用edge n-gram，如果用widely known order，用completion suggester**：
> When you need search-as-you-type for text which has a widely known order, such as movie or song titles, the completion suggester is a much more efficient choice than edge N-grams. Edge N-grams have the advantage when trying to autocomplete words that can appear in any order.

不过什么是widely known order？ TODO

看起来它也是一个field type，和"search-as-you-type"一样离谱……
- https://www.elastic.co/guide/en/elasticsearch/reference/current/search-suggesters.html#completion-suggester

**一个很大的优势在于，它被优化的巨快，甚至直接放到了内存里**：
> Ideally, auto-complete functionality should be as fast as a user types to provide instant feedback relevant to what a user has already typed in. Hence, completion suggester is optimized for speed. The suggester uses data structures that enable fast lookups, but are costly to build and are stored in-memory.

## search analyzer - 和edge n-gram一起使用
**正常情况下，搜索时对搜索词使用的analyzer，应该和构建索引时使用的analyzer一样，这样搜索词token和索引里的待搜索token才能在同一套标准下进行匹配**。比如构建索引时使用english analyzer，搜索时也使用english analyzer。

但是edge n-gram不一样，它存的时候用到的analyzer是edge n-gram相关的，存的是前缀。**但是搜索的时候，不能对搜索词也应用这个analyzer，否则只要搜索词的某个前缀和索引词的某个前缀相同，就能被搜出来**：

比如索引词是pikachu，存在倒排索引里的是pi/pik/pika三个前缀。搜索词是pipi，理论上来讲它不应该搜出来pikachu。但是如果对它也应用edge n-gram，搜索词也变成了三个：pi/pip/pipi。其中，pi这个搜索词是可以匹配上倒排索引里的pi的。但这完全不是我们想要的结果。

`analyzer`和`search_analyzer`都是定义mapping时，field的属性：
- **前者叫index analyzer**；
- **后者叫search analyzer**；

定义一个有多个名字的field：
```
      "name":{
        "type":"keyword",
        "fields":{
          "standard":{
            "type":"text",
            "analyzer":"standard"
          },
          "autocomplete":{
            "type":"text",
            "analyzer":"autocomplete_sentence",
            "search_analyzer":"autocomplete_sentence_search"
          }
        }
      },
```
`name.autocomplete`这个field使用自定义的`autocomplete_sentence` analyzer作为index analyzer，使用自定义的`autocomplete_sentence_search`作为search analyzer。

前者使用edge n-gram，后者就是单纯的keyword tokenizer：
```
        "analyzer":{
          "autocomplete_sentence":{
            "tokenizer":"sentence_edge_ngram",
            "filter":[
              "lowercase"
            ]
          },
          "autocomplete_sentence_search":{
            "tokenizer":"keyword",
            "filter":[
              "lowercase"
            ]
          }
        },
        "tokenizer":{
          "sentence_edge_ngram":{
            "type":"edge_ngram",
            "min_gram":2,
            "max_gram":20,
            "token_chars":[
              
            ]
          }
        }
      }
```
- https://www.elastic.co/guide/en/elasticsearch/reference/current/search-analyzer.html

## shingle - token滑动窗口
shingle是token层级的n-gram：**edge n-gram token filter是给一个token的前缀做ngram，shingle是给多个token做ngram**。

比如：
```
GET /_analyze
{
  "tokenizer": "whitespace",
  "filter": [
    {
      "type": "shingle",
      "min_shingle_size": 2,
      "max_shingle_size": 3
    }
  ],
  "text": "i love you bibi"
}
```
能生成4个单独的word，和5个shingle：
```
{
  "tokens" : [
    {
      "token" : "i",
      "start_offset" : 0,
      "end_offset" : 1,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "i love",
      "start_offset" : 0,
      "end_offset" : 6,
      "type" : "shingle",
      "position" : 0,
      "positionLength" : 2
    },
    {
      "token" : "i love you",
      "start_offset" : 0,
      "end_offset" : 10,
      "type" : "shingle",
      "position" : 0,
      "positionLength" : 3
    },
    {
      "token" : "love",
      "start_offset" : 2,
      "end_offset" : 6,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : "love you",
      "start_offset" : 2,
      "end_offset" : 10,
      "type" : "shingle",
      "position" : 1,
      "positionLength" : 2
    },
    {
      "token" : "love you bibi",
      "start_offset" : 2,
      "end_offset" : 15,
      "type" : "shingle",
      "position" : 1,
      "positionLength" : 3
    },
    {
      "token" : "you",
      "start_offset" : 7,
      "end_offset" : 10,
      "type" : "word",
      "position" : 2
    },
    {
      "token" : "you bibi",
      "start_offset" : 7,
      "end_offset" : 15,
      "type" : "shingle",
      "position" : 2,
      "positionLength" : 2
    },
    {
      "token" : "bibi",
      "start_offset" : 11,
      "end_offset" : 15,
      "type" : "word",
      "position" : 3
    }
  ]
}
```

**使用shingle做索引的field，搜索的时候使用同样的analyzer就行了，不像edge n-gram必须设置不同的search analyzer，因为shingle本来就是按照单词匹配的，符合对搜索的认知**。

- https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-shingle-tokenfilter.html

设置带有shingle的analyzer：
```
# analyzer
PUT /puzzle
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 1,
    "analysis": {
      "char_filter": {
        "&_to_and": {
          "type":       "mapping",
          "mappings": [ "&=> and "]
        }
      },
      "filter": {
        "english_stop": {
          "type":       "stop",
          "ignore_case": true,
          "stopwords":  ["a", "an", "are", "as", "at", "be", "but", "by", "for", "if", "in", "into", "is", "it", "no", "not", "of", "on", "or", "such", "that", "the", "their", "then", "there", "these", "they", "this", "to", "was", "will", "with"]
        },
        "english_keywords": {
          "type":       "keyword_marker",
          "keywords":   ["example"]
        },
        "english_stemmer": {
          "type":       "stemmer",
          "language":   "english"
        },
        "english_possessive_stemmer": {
          "type":       "stemmer",
          "language":   "possessive_english"
        },
        "my_shingle_filter": {
                    "type":             "shingle",
                    "min_shingle_size": 2,
                    "max_shingle_size": 2,
                    "output_unigrams":  false  
                }
      },
      "analyzer": {
        "reb_standard": {
          "type":         "custom",
          "char_filter":  [ "&_to_and" ],
          "tokenizer":    "standard"
        },
        "reb_english": {
          "type":         "custom",
          "char_filter":  [ "&_to_and" ],
          "tokenizer":    "standard",
          "filter":       [ "english_possessive_stemmer",
            "lowercase",
            "english_stop",
            "english_keywords",
            "english_stemmer" ]
        },
        "my_shingle_analyzer": {
                    "type":             "custom",
                    "char_filter":  [ "&_to_and" ],
                    "tokenizer":        "standard",
                    "filter": [
                        "lowercase",
                        "my_shingle_filter"
                    ]
        },
        "eng_shingle_analyzer": {
                    "type":             "custom",
                    "char_filter":  [ "&_to_and" ],
                    "tokenizer":        "standard",
                    "filter": [
                      "english_possessive_stemmer",
                      "lowercase",
                      "english_stop",
                      "english_keywords",
                      "english_stemmer",
                      "my_shingle_filter"
                    ]
        }
      }
    }
  }
}
```
设置mapping：
```
# property
PUT /puzzle/_mapping
{
  "properties": {
    "article": {
      "type": "keyword",
      "fields": {
        "stan": {
          "type": "text",
          "analyzer": "standard"
        },
        "eng": {
          "type": "text",
          "analyzer": "english"
        },
        "reb_eng": {
          "type": "text",
          "analyzer": "reb_english"
        },
        "reb_stan": {
          "type": "text",
          "analyzer": "reb_standard"
        },
        "icu": {
          "type": "text",
          "analyzer": "icu_analyzer"
        },
        "ik": {
          "type": "text",
          "analyzer": "ik_smart"
        }
      }
    }
  }
}
 
PUT /puzzle/_mapping
{
  "properties": {
    "article": {
      "type": "keyword",
      "fields": {
        "shingle": {
          "type": "text",
          "analyzer": "eng_shingle_analyzer"
        }
      }
    }
  }
}
```
搜索：
```
GET /puzzle/_search
{
  "query": {
      "bool": {
         "must": {
            "match": {
               "article.reb_eng": {
                  "query": "puzzles & survival",
                  "minimum_should_match": "100%"
               }
            }
         },
         "should": [
           {
              "match": {
                 "article.shingle": "puzzles & survival"
              }
           }
         ]
      }
   }
}
```

**shingle介于match和match_phrase之间**：它像match一样不要求所有搜索词都出现（match_phrase虽然可以调slop，但是所有词必须出现），同时不像match丝毫不考虑顺序（shingle考虑了局部顺序，单词局部顺序和shingle匹配的得分会高）：
- https://www.elastic.co/guide/cn/elasticsearch/guide/current/shingles.html

# highlight
highlight能给搜索结果高亮，方便发现为什么和文档匹配了。尤其是在变形词、带slop的match phrase匹配的场合：
- https://www.elastic.co/guide/en/elasticsearch/reference/current/highlighting.html

highlight需要指定高亮哪些field，一般是用于搜索的field：
```
GET /<index>/_search
{
  "query": {
    "match": {
      "description.reb_eng": "pikachu"
    }
  },
  "highlight": {
    "fields": {
      "description.reb_eng": {}
    }
  },
  "size": 10
}
```
**highlight默认不会给整个field highlight，而是每次highlight一小段文本（fragment），一共返回5小段**。比如：
```
        "highlight" : {
          "description.reb_eng" : [
            """pubg,
<em>pikachu</em> gaming handcam,
<em>pikachu</em> gaming live,
<em>pikachu</em> gaming yt,
<em>pikachu</em> gaming tdm,
<em>pikachu</em> gaming""",
            """bablu <em>pikachu</em> gaming,
bia <em>pikachu</em> gaming,
black <em>pikachu</em> gaming,
<em>pikachu</em> gaming channel,
<em>pikachu</em> gaming""",
            """,
<em>pikachu</em> gaming device,
dr <em>pikachu</em> gaming,
dnm <em>pikachu</em> gaming,
dac <em>pikachu</em> gaming,
<em>pikachu</em> gaming ff""",
            """montage <em>pikachu</em> gaming,
<em>pikachu</em> gaming intro,
<em>pikachu</em> gaming id,
gaming with kev <em>pikachu</em>,
king <em>pikachu</em>""",
            """<em>pikachu</em>,
viano gaming <em>pikachu</em>,
<em>pikachu</em> gaming vn,
gaming with <em>pikachu</em>,
<em>pikachu</em> gaming xt,
<em>pikachu</em> 01"""
          ]
        }
```
控制highlight行为的参数：
- `fragment_size`：每一小段的长度，默认100个字符。会在超出100的单词的边界处截断。所以实际一小段可能会比100大点儿；
- `number_of_fragments`：返回几小段。默认是5；

`fragment_size`设为0可以不切分。也就是说从匹配到关键词的句子一直到末尾都会作为一整段。**但是之前的句子则不会被返回**。

**如果想返回整段内容，需要设置`number_of_fragments=0`**：
> The maximum number of fragments to return. **If the number of fragments is set to 0, no fragments are returned. Instead, the entire field contents are highlighted and returned.** This can be handy when you need to highlight short texts such as a title or address, but fragmentation is not required. If number_of_fragments is 0, fragment_size is ignored.


```
GET /<index>/_search
{
  "query": {
    "match": {
      "description.reb_eng": "pikachu"
    }
  },
  "highlight": {
    "number_of_fragments": 0, 
    "fields": {
      "description.reb_eng": {}
    }
  },
  "size": 10
}
```
一个比较极端的例子：使用"`pikachu`"搜索"`【去吧！】pikachu go`"，如果按照默认行为（只返回匹配到的分片，而不是返回整段），前面的"`【去吧！】`"不会返回，只会返回"`<em>pikachu</em> go`"，应该是中文字符被当做句子分界了（分界默认用的是java的BreakIterator）：
> Break highlighted fragments at the next sentence boundary, as determined by Java’s [BreakIterator](https://docs.oracle.com/javase/8/docs/api/java/text/BreakIterator.html).

如果想返回完整的"`【去吧！】<em>pikachu</em> go`"，就要设置`number_of_fragments=0`。
